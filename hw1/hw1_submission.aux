\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Behavioral Cloning (9.75 pt)}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Part 2 (1.5 pt)}{1}{subsection.1.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Report your result in this table.}}{1}{table.1}\protected@file@percent }
\newlabel{tab:p2}{{1}{1}{Report your result in this table}{table.1}{}}
\newlabel{tab:p2@cref}{{[table][1][]1}{[1][1][]1}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Part 3 (5.25 pt)}{1}{subsection.1.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Fill your results in this table, listing hyperparameters in this caption.}}{1}{table.2}\protected@file@percent }
\newlabel{tab:p3}{{2}{1}{Fill your results in this table, listing hyperparameters in this caption}{table.2}{}}
\newlabel{tab:p3@cref}{{[table][2][]2}{[1][1][]1}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Part 4 (3 pt)}{1}{subsection.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Behavioral cloning (BC) performance on \textbf  {Ant-v2} as a function of the \emph  {number of training steps per iteration}. Points show the evaluation mean return over at least five rollouts with standard-deviation error bars. Increasing the training steps dramatically improves performance from severe underfitting (250 steps) to near-expert performance around 3–4k steps, after which gains taper off.}}{1}{figure.1}\protected@file@percent }
\newlabel{fig:p4}{{1}{1}{Behavioral cloning (BC) performance on \textbf {Ant-v2} as a function of the \emph {number of training steps per iteration}. Points show the evaluation mean return over at least five rollouts with standard-deviation error bars. Increasing the training steps dramatically improves performance from severe underfitting (250 steps) to near-expert performance around 3–4k steps, after which gains taper off}{figure.1}{}}
\newlabel{fig:p4@cref}{{[figure][1][]1}{[1][1][]1}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}DAgger (5.25 pt)}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Part 2 (5.25 pt)}{2}{subsection.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces DAgger performance on \textbf  {Ant-v2} (left) and \textbf  {Humanoid-v2} (right). Curves plot evaluation mean return versus DAgger iteration, with error bars showing the standard deviation across evaluation rollouts. Horizontal dashed/dotted lines show the expert policy and behavioral cloning (BC) baselines, respectively. Setup: DAgger used a 3-layer MLP (\texttt  {--n\_layers 3}), learning rate $4\times 10^{-3}$, evaluation batch size 5000; expert datasets from \texttt  {rob831/expert\_data} (Ant-v2 and Humanoid-v2). BC baselines were trained with the same learning rate and (for Ant) a 5-layer MLP.}}{2}{figure.2}\protected@file@percent }
\newlabel{fig:p5}{{2}{2}{DAgger performance on \textbf {Ant-v2} (left) and \textbf {Humanoid-v2} (right). Curves plot evaluation mean return versus DAgger iteration, with error bars showing the standard deviation across evaluation rollouts. Horizontal dashed/dotted lines show the expert policy and behavioral cloning (BC) baselines, respectively. Setup: DAgger used a 3-layer MLP (\texttt {--n\_layers 3}), learning rate $4\times 10^{-3}$, evaluation batch size 5000; expert datasets from \texttt {rob831/expert\_data} (Ant-v2 and Humanoid-v2). BC baselines were trained with the same learning rate and (for Ant) a 5-layer MLP}{figure.2}{}}
\newlabel{fig:p5@cref}{{[figure][2][]2}{[1][2][]2}{}{}{}}
\gdef \@abspage@last{2}
